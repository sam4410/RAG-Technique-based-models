{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIa9A7+SmBrwofoa9yHTXn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sam4410/RAG-Technique-based-models/blob/main/Hybrid_Adaptive_RAG_Driven_Gen_AI_with_Expert_Human_Feedback.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be implementing Adaptive RAG with Human Feedback (HF) loop. This system is called adaptive because the documents used for retrieval are updated. Integrating HF in RAG leads to a pragmatic hybrid approach because it involves humans in an otherwise automated generative process.\n",
        "\n",
        "Implementation of Adaptive RAG framework is goign to include the following:\n",
        "* Defining the adaptive RAG ecosystem\n",
        "* Applying adaptive RAG to augmented retrieval queries\n",
        "* Automating augmented generative AI inputs with HF\n",
        "* Automating end-user feedback rankings to trigger expert HF\n",
        "* Creating an automated feedback system for a human expert\n",
        "* Integrating HF with adaptive RAG for GPT-4o\n",
        "\n",
        "We will build our own pipeline and introduce HF. The program is divided into three separate parts: the retriever, generator, and evaluator functions, which can be separate agents in a real-life project's pipeline"
      ],
      "metadata": {
        "id": "WhUWD0YgTYi-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Retriever"
      ],
      "metadata": {
        "id": "taN5VxoCbNNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests==2.32.3 beautifulsoup4==4.12.3 openai==1.40.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPPvoZnYUYuZ",
        "outputId": "30dd163d-e7a7-4312-cda0-b18bcd9288bc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests==2.32.3 in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4==4.12.3 in /usr/local/lib/python3.11/dist-packages (4.12.3)\n",
            "Requirement already satisfied: openai==1.40.3 in /usr/local/lib/python3.11/dist-packages (1.40.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests==2.32.3) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests==2.32.3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests==2.32.3) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests==2.32.3) (2025.1.31)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4==4.12.3) (2.6)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.40.3) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.40.3) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.40.3) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.40.3) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.40.3) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai==1.40.3) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai==1.40.3) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai==1.40.3) (4.12.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai==1.40.3) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.40.3) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai==1.40.3) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai==1.40.3) (2.27.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import warnings\n",
        "import requests\n",
        "from google.colab import userdata\n",
        "from bs4 import BeautifulSoup\n",
        "from huggingface_hub import login\n",
        "import re\n",
        "import openai\n",
        "from google.colab import drive\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Connect this Colab to my Google Drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "#Retrieving and setting OpenAI API key\n",
        "f = open(\"drive/MyDrive/Colab Notebooks/key_files/openai_api_key.txt\", \"r\")\n",
        "API_KEY=f.readline().strip()\n",
        "f.close()\n",
        "\n",
        "#The OpenAI API key\n",
        "os.environ['OPENAI_API_KEY'] =API_KEY\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "#Retrieving and setting Activeloop API token\n",
        "f = open(\"drive/MyDrive/Colab Notebooks/key_files/activeloop_token.txt\", \"r\")\n",
        "API_token=f.readline().strip()\n",
        "f.close()\n",
        "ACTIVELOOP_TOKEN=API_token\n",
        "os.environ['ACTIVELOOP_TOKEN'] =ACTIVELOOP_TOKEN\n",
        "\n",
        "# signing to hugging face hub\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lH2zofHurN5",
        "outputId": "a05f6b19-f539-464c-fa1d-e3dc0449b44c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preparing the dataset"
      ],
      "metadata": {
        "id": "jY7FM0WrbucE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we will retrieve Wikipedia documents by scraping them through their URLs. The dataset will contain automated or human-crafted labels for each document, which is the first step\n",
        "# toward indexing the documents of a dataset\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# URLs of the Wikipedia articles mapped to keywords\n",
        "urls = {\n",
        "    \"prompt engineering\": \"https://en.wikipedia.org/wiki/Prompt_engineering\",\n",
        "    \"artificial intelligence\":\"https://en.wikipedia.org/wiki/Artificial_intelligence\",\n",
        "    \"llm\": \"https://en.wikipedia.org/wiki/Large_language_model\",\n",
        "    \"llms\": \"https://en.wikipedia.org/wiki/Large_language_model\"\n",
        "    }"
      ],
      "metadata": {
        "id": "ljiyk7M1bpI7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Processing the data"
      ],
      "metadata": {
        "id": "0uyRu57Zc_8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# apply a standard scraping and text-cleaning function to the document that will be retrieved\n",
        "def fetch_and_clean(url):\n",
        "  # Fetch the content of the URL\n",
        "  response = requests.get(url)\n",
        "  soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "  # Find the main content of the article, ignoring side boxes and headers\n",
        "  content = soup.find('div', {'class': 'mw-parser-output'})\n",
        "\n",
        "  # Remove less relevant sections such as \"See also\", \"References\", etc.\n",
        "  for section_title in ['References', 'Bibliography', 'External links', 'See also']:\n",
        "    section = content.find('span', {'id': section_title})\n",
        "    if section:\n",
        "      for sib in section.parent.find_next_siblings():\n",
        "        sib.decompose()\n",
        "      section.parent.decompose()\n",
        "\n",
        "  # Focus on extracting and cleaning text from paragraph tags only\n",
        "  paragraphs = content.find_all('p')\n",
        "  cleaned_text = ' '.join(paragraph.get_text(separator=' ', strip=True) for paragraph in paragraphs)\n",
        "  cleaned_text = re.sub(r'\\[\\d+\\]', '', cleaned_text)   # remove citation markers like [1], [2], etc\n",
        "\n",
        "  return cleaned_text"
      ],
      "metadata": {
        "id": "bXyM7TLfdD7S"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Retrieval process for user input\n",
        "\n",
        "The first step here involves identifying a keyword within the user's input. The function \"process_query\"\n",
        "takes two parameters: user_input and num_words. The number of words to retrieve is restricted by\n",
        "factors like the input limitations of the model, cost considerations, and overall system performance"
      ],
      "metadata": {
        "id": "xB8QSsOqep_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "def process_query(user_input, num_words):\n",
        "  user_input = user_input.lower()\n",
        "\n",
        "  # Check for any of the specified keywords in the input\n",
        "  matched_keyword = next((keyword for keyword in urls if keyword in user_input), None)\n",
        "\n",
        "  # upon finding match between keyword in user query and keywords associated with URLs\n",
        "  # following functions for fetching and cleaning the data are triggered\n",
        "  if matched_keyword:\n",
        "    print(f\"Fetching data from: {urls[matched_keyword]}\")\n",
        "    cleaned_text = fetch_and_clean(urls[matched_keyword])\n",
        "\n",
        "    # Limit the display to the specified number of words from the cleaned text\n",
        "    words = cleaned_text.split()   # Split the text into words\n",
        "    first_n_words = ' '.join(words[:num_words])   # Join the first n words into a single string\n",
        "\n",
        "    # num_words parameter helps in chunking the text\n",
        "    # cleaned and truncated text is then formatted for display\n",
        "    # Wrap the first n words to 80 characters wide for display\n",
        "    wrapped_text = textwrap.fill(first_n_words, width=80)\n",
        "    print(\"\\nFirst {} words of the cleaned text:\".format(num_words))\n",
        "    print(wrapped_text) # Print the first n words as a well-formatted paragraph\n",
        "\n",
        "    # Use the exact same first_n_words for the GPT-4 prompt to ensure consistency\n",
        "    prompt = f\"Summarize the following information about {matched_keyword}:\\n{first_n_words}\"\n",
        "\n",
        "    wrapped_prompt = textwrap.fill(prompt, width=80) # Wrap prompt text\n",
        "    print(\"\\nPrompt for Generator:\", wrapped_prompt)\n",
        "\n",
        "    # Return the specified number of words\n",
        "    return first_n_words\n",
        "  else:\n",
        "    print(\"No relevant keywords found. Please enter a query related to 'LLM', 'LLMs', or 'Prompt Engineering'.\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "Bnk5ds3OeoxS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above function ultimately returns the first n words, providing a concise and relevant snippet\n",
        "of information based on the user's query. This design allows the system to manage data retrieval efficiently\n",
        "while also maintaining user engagement"
      ],
      "metadata": {
        "id": "gXsd0-e5ho76"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.Generator"
      ],
      "metadata": {
        "id": "UWSz2d6Bhz_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1. Integrating HF-RAG for augmented document inputs\n",
        "\n",
        "The dynamic nature of information retrieval and the necessity for contextually relevant data augmentation in generative AI models require a flexible system capable of adapting to varying levels of input quality.\n",
        "\n",
        "The adaptive RAG selection system employs HF scores to determine the\n",
        "optimal retrieval strategy for document implementation within the RAG ecosystem.\n",
        "\n",
        "The adaptive approach aims to optimize the balance between automated retrieval and human insight,\n",
        "ensuring the generative model's outputs are of the highest possible relevance and accuracy"
      ],
      "metadata": {
        "id": "XN-9CpGwjGzj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2. Input\n",
        "\n",
        "A user of Company C is prompted to enter a question:"
      ],
      "metadata": {
        "id": "td3-R8I0lGs8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Request user input for keyword parsing\n",
        "user_input = input(\"Enter your query: \").lower()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0U-3qAqhoFr",
        "outputId": "ba1600ec-dd77-4bea-eb9c-3b98ca45f140"
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your query: What is LLMs?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.3. Mean Ranking Simulation Scenario\n",
        "\n",
        "Let's assume that the human user feedback panel has been evaluating\n",
        "the hybrid adaptive RAG system for some time. The user feedback panel ranks the reponses a number of times, which automatically updates by calculating the mean of the ratings and storing it in a ranking variable named 'ranking'. The ranking score will help the management team decide whether to downgrade the rank of a document, upgrade it, or suppress documents through manual or automated functions."
      ],
      "metadata": {
        "id": "xdOMA7Y8lXdW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will begin with a 1 to 5 ranking, which will deactivate RAG so that we can see the native response of the generative model\n",
        "#Select a score between 1 and 5 to run the simulation\n",
        "ranking=5"
      ],
      "metadata": {
        "id": "kXx7l4qQk2UQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we will modify this value to activate RAG without additional human-expert feedback with\n",
        "ranking=5. Finally, we will modify this value to activate human feedback RAG without retrieving\n",
        "documents with ranking=3."
      ],
      "metadata": {
        "id": "9O_98kt-m9ot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initializing the text for the generative AI model simulations\n",
        "text_input=[]"
      ],
      "metadata": {
        "id": "4NSTofD0m8vT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each time we switch scenarios, make sure to come back and reinitialize text_input"
      ],
      "metadata": {
        "id": "S2dIyvV-nL9G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ranking 1–2: No RAG"
      ],
      "metadata": {
        "id": "f_JR29MmnQVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ranking of the generative AI’s output is very low. All RAG functionality is deactivated until the management team can analyze and improve the system\n",
        "# In this case, text_input is equal to user_input\n",
        "if ranking >= 1 and ranking < 3:\n",
        "  text_input = user_input"
      ],
      "metadata": {
        "id": "KUOcex5gnDc4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ranking 3–4: Human-expert feedback RAG\n",
        "\n",
        "In this scenario, human-expert feedback was triggered by poor user feedback ratings with automated RAG documents(ranking=5) and without RAG (ranking\n",
        "1-2). The human-expert panel has filled in a flashcard, which has now been stored as an expert-level\n",
        "RAG document."
      ],
      "metadata": {
        "id": "XXuagZbWqSyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# program first checks the ranking and activates HF retrieval\n",
        "hf=False\n",
        "if ranking>3 and ranking<5:\n",
        "  hf=True"
      ],
      "metadata": {
        "id": "TpuMPgkkqQvV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The program will then fetch the proper document from an expert panel (selected experts within a\n",
        "corporation) dataset based on keywords, embeddings, or other search methods that fit the goals of a\n",
        "project. In this case, we assume we have found the right flashcard and download it"
      ],
      "metadata": {
        "id": "iT-maTLdrHI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"drive/MyDrive/human_feedback.txt\", \"r\") as file:\n",
        "  content = file.read().replace('\\n', ' ').replace('#', '')     #Removing new line and markdown characters\n",
        "text_input=content\n",
        "print(text_input)  #The content of the file explains both what an LLM is and how it can help Company C improve customer support"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "go_0oRhDuRnh",
        "outputId": "cf755af8-baa4-4e2c-9180-aa98a07a2e30"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A Large Language Model (LLM) is an advanced AI system trained on vast amounts of text data to generate human-like text responses. It understands and generates language based on the patterns and information it has learned during training. LLMs are highly effective in various language-based tasks, including answering questions, making recommendations, and facilitating conversations. They can be continually updated with new information and trained to understand specific domains or industries.For the C-phone series customer support, incorporating an LLM could significantly enhance service quality and efficiency. The conversational agent powered by an LLM can provide instant responses to customer inquiries, reducing wait times and freeing up human agents for more complex issues. It can be programmed to handle common technical questions about the C-phone series, troubleshoot problems, guide users through setup processes, and offer tips for optimizing device performance. Additionally, it can be used to gather customer feedback, providing valuable insights into user experiences and product performance. This feedback can then be used to improve products and services. Furthermore, the LLM can be designed to escalate issues to human agents when necessary, ensuring that customers receive the best possible support at all levels. The agent can also provide personalized recommendations for customers based on their usage patterns and preferences, enhancing user satisfaction and loyalty. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If we try generate content based this text_input creatd as symbol of expert human feedabck, the response is satisfactory\n",
        "# The preceding response is now much better since it defines LLMs and also shows how to improve customer service for Company C's C-phone series."
      ],
      "metadata": {
        "id": "BcwgZJ5HvuEY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ranking 5: RAG with no human-expert feedback documents\n",
        "\n",
        "In case, users do not require RAG documents that include human-expert RAG flashcards, snippets, or\n",
        "documents. This might be the case, particularly, if software engineers are the users. In this case, the maximum number of words is limited to 100 to optimize API costs, but can be modified"
      ],
      "metadata": {
        "id": "Lwj1ER0JxSIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if ranking >= 5:\n",
        "  max_words=100 #Limit: the size of the data we can add to the input\n",
        "  rdata=process_query(user_input,max_words)\n",
        "  if rdata:\n",
        "    rdata_clean = rdata.replace('\\n', ' ').replace('#', '')\n",
        "    rdata_sentences = rdata_clean.split('. ')\n",
        "    print(rdata)\n",
        "  text_input=rdata\n",
        "  print(text_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xI_G_tpExO8_",
        "outputId": "50bb1019-4384-4af0-8bef-dd4ac09e5bd8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching data from: https://en.wikipedia.org/wiki/Large_language_model\n",
            "\n",
            "First 100 words of the cleaned text:\n",
            "A large language model ( LLM ) is a type of machine learning model designed for\n",
            "natural language processing tasks such as language generation . LLMs are\n",
            "language models with many parameters, and are trained with self-supervised\n",
            "learning on a vast amount of text. The largest and most capable LLMs are\n",
            "generative pretrained transformers (GPTs). Modern models can be fine-tuned for\n",
            "specific tasks or guided by prompt engineering . [ 1 ] These models acquire\n",
            "predictive power regarding syntax , semantics , and ontologies [ 2 ] inherent in\n",
            "human language corpora, but they also inherit inaccuracies and biases present\n",
            "\n",
            "Prompt for Generator: Summarize the following information about llm: A large language model ( LLM ) is\n",
            "a type of machine learning model designed for natural language processing tasks\n",
            "such as language generation . LLMs are language models with many parameters, and\n",
            "are trained with self-supervised learning on a vast amount of text. The largest\n",
            "and most capable LLMs are generative pretrained transformers (GPTs). Modern\n",
            "models can be fine-tuned for specific tasks or guided by prompt engineering . [\n",
            "1 ] These models acquire predictive power regarding syntax , semantics , and\n",
            "ontologies [ 2 ] inherent in human language corpora, but they also inherit\n",
            "inaccuracies and biases present\n",
            "A large language model ( LLM ) is a type of machine learning model designed for natural language processing tasks such as language generation . LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text. The largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering . [ 1 ] These models acquire predictive power regarding syntax , semantics , and ontologies [ 2 ] inherent in human language corpora, but they also inherit inaccuracies and biases present\n",
            "A large language model ( LLM ) is a type of machine learning model designed for natural language processing tasks such as language generation . LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text. The largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering . [ 1 ] These models acquire predictive power regarding syntax , semantics , and ontologies [ 2 ] inherent in human language corpora, but they also inherit inaccuracies and biases present\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.6. Content Generation"
      ],
      "metadata": {
        "id": "46GXXft2zfvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "import time\n",
        "import httpx\n",
        "from openai import DefaultHttpxClient\n",
        "\n",
        "client = OpenAI(http_client=DefaultHttpxClient())\n",
        "gptmodel=\"gpt-4o\"\n",
        "start_time = time.time() # Start timing before the request\n",
        "\n",
        "# define a standard Gpt-4o prompt, giving it enough information to respond and leaving the rest up to the model and RAG data\n",
        "def call_gpt4_with_full_text(itext):\n",
        "  # Join all lines to form a single string\n",
        "  text_input = '\\n'.join(itext)\n",
        "  prompt = f\"Please summarize or elaborate on the following content:\\n{text_input}\"\n",
        "\n",
        "  try:\n",
        "    response = client.chat.completions.create(\n",
        "        model=gptmodel,\n",
        "        messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert Natural Language Processing exercise expert.\"},\n",
        "                {\"role\": \"assistant\", \"content\": \"1.You can explain read the input and answer in detail\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "    ],\n",
        "    temperature=0.1 # Add the temperature parameter here and otherparameters you need\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "  except Exception as e:\n",
        "    return str(e)"
      ],
      "metadata": {
        "id": "wh8gF41DyKW6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function and print the result\n",
        "gpt4_response = call_gpt4_with_full_text(text_input)\n",
        "\n",
        "response_time = time.time() - start_time  # Measure response time\n",
        "print(f\"Response Time: {response_time:.2f} seconds\")  # Print response time\n",
        "\n",
        "print(gptmodel,\"Response:\", gpt4_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbXgR1865nIH",
        "outputId": "16bb23dc-d2d5-4092-b65e-8ea3f337ab34"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response Time: 44.72 seconds\n",
            "gpt-4o Response: A large language model (LLM) is a type of machine learning model specifically designed for natural language processing (NLP) tasks, such as language generation. These models are characterized by having a large number of parameters and are trained using self-supervised learning on extensive text datasets. The most advanced and capable LLMs are often generative pre-trained transformers (GPTs). Modern LLMs can be fine-tuned for specific tasks or guided using prompt engineering to achieve desired outcomes.\n",
            "\n",
            "These models gain predictive power in understanding syntax, semantics, and ontologies inherent in human language corpora. However, they also inherit inaccuracies and biases present in the data they are trained on. This means that while they can perform complex language tasks, they may also reflect and perpetuate existing biases and errors found in the training data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# below code then formats the output\n",
        "import textwrap\n",
        "\n",
        "def print_formatted_response(response):\n",
        "  # Define the width for wrapping the text\n",
        "  wrapper = textwrap.TextWrapper(width=80)\n",
        "  wrapped_text = wrapper.fill(text=response)\n",
        "\n",
        "  # Print the formatted response with a header and footer\n",
        "  print(\"GPT-4 Response:\")\n",
        "  print(\"---------------\")\n",
        "  print(wrapped_text)\n",
        "  print(\"---------------\\n\")"
      ],
      "metadata": {
        "id": "laAIequv1Qbm"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Assuming 'gpt4_response' contains the response from the previous GPT-4 call\n",
        "print_formatted_response(gpt4_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEcIt21_12Ea",
        "outputId": "2d2a0fd0-34b8-40cb-e930-7ab00c3be6ac"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT-4 Response:\n",
            "---------------\n",
            "A large language model (LLM) is a type of machine learning model specifically\n",
            "designed for natural language processing (NLP) tasks, such as language\n",
            "generation. These models are characterized by having a large number of\n",
            "parameters and are trained using self-supervised learning on extensive text\n",
            "datasets. The most advanced and capable LLMs are often generative pre-trained\n",
            "transformers (GPTs). Modern LLMs can be fine-tuned for specific tasks or guided\n",
            "using prompt engineering to achieve desired outcomes.  These models gain\n",
            "predictive power in understanding syntax, semantics, and ontologies inherent in\n",
            "human language corpora. However, they also inherit inaccuracies and biases\n",
            "present in the data they are trained on. This means that while they can perform\n",
            "complex language tasks, they may also reflect and perpetuate existing biases and\n",
            "errors found in the training data.\n",
            "---------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.Evaluator\n",
        "\n",
        "we will implement two automatic metrics:\n",
        "response time and cosine similarity score. We will then implement two interactive evaluation functions:\n",
        "human user rating and human-expert evaluation."
      ],
      "metadata": {
        "id": "so-ZvHy33dGQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.1. Response time\n"
      ],
      "metadata": {
        "id": "42V2gz-V3xxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time() # Start timing before the request\n",
        "\n",
        "response_time = time.time() - start_time # Measure response time\n",
        "print(f\"Response Time: {response_time:.2f} seconds\") # Print response time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2PphBU93hMC",
        "outputId": "eece7a74-430f-474d-fb7a-af8b9056ad8c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response Time: 0.00 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2. Cosine similarity score"
      ],
      "metadata": {
        "id": "ZiegCOpB6UPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def calculate_cosine_similarity(text1, text2):\n",
        "  vectorizer = TfidfVectorizer()\n",
        "  tfidf = vectorizer.fit_transform([text1, text2])\n",
        "  similarity = cosine_similarity(tfidf[0:1], tfidf[1:2])\n",
        "  return similarity[0][0]\n",
        "\n",
        "# Example usage with your existing functions\n",
        "similarity_score = calculate_cosine_similarity(text_input, gpt4_response)\n",
        "print(f\"Cosine Similarity Score: {similarity_score:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4BPN1gW6Hw0",
        "outputId": "316ce29c-9f7c-4572-9275-591915483d7a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity Score: 0.734\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The score shows a strong similarity between the input and the output of the model. But how will a\n",
        "human user rate this response? Let's find out.\n",
        "\n",
        "#### 3.3. Human user rating\n",
        "\n",
        "The human user rating interface provides human user feedback. it is recommended designing this interface and process after fully understanding user needs through a workshop with them"
      ],
      "metadata": {
        "id": "f_wtvegs68_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code begins with the interface's parameters\n",
        "# Score parameters\n",
        "counter=20         # number of feedback queries\n",
        "score_history=30   # human feedback\n",
        "threshold=4        # minimum rankings to trigger human expert feedback"
      ],
      "metadata": {
        "id": "EUCQvJhQ6t0w"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this simulation, the parameters show that the system has computed human feedback:\n",
        "* counter=20 shows the number of ratings already entered by the users\n",
        "* score_history=60 shows the total score of the 20 ratings\n",
        "* threshold=4 states the minimum mean rating, score_history/counter, to obtain without triggering a human-expert feedback request"
      ],
      "metadata": {
        "id": "BslD5lxi8RAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def evaluate_response(response):\n",
        "  print(\"\\nGenerated Response:\")\n",
        "  print(response)\n",
        "  print(\"\\nPlease evaluate the response based on the following criteria:\")\n",
        "  print(\"1 - Poor, 2 - Fair, 3 - Good, 4 - Very Good, 5 - Excellent\")\n",
        "  score = input(\"Enter the relevance and coherence score (1-5): \")\n",
        "  try:\n",
        "    score = int(score)\n",
        "    if 1 <= score <= 5:\n",
        "      return score\n",
        "    else:\n",
        "      print(\"Invalid score. Please enter a number between 1 and 5.\")\n",
        "      return evaluate_response(response)      # Recursive call if the input if the input is invalid\n",
        "  except ValueError:\n",
        "    print(\"Invalid input. Please enter a number.\")\n",
        "    return evaluate_response(response)      # Recursive call if the input is invalid\n",
        "score = evaluate_response(gpt4_response)\n",
        "print(\"Evaluator Score:\", score)\n",
        "\n",
        "counter+=1\n",
        "score_history+=score\n",
        "mean_score=round(np.mean(score_history/counter), 2)\n",
        "if counter>0:\n",
        "  print(\"Rankings      :\", counter)\n",
        "  print(\"Score history : \", mean_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k17w9SU18ytn",
        "outputId": "5007fb28-aec5-493d-c51a-2160e9e6591f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Response:\n",
            "A large language model (LLM) is a type of machine learning model specifically designed for natural language processing (NLP) tasks, such as language generation. These models are characterized by having a large number of parameters and are trained using self-supervised learning on extensive text datasets. The most advanced and capable LLMs are often generative pre-trained transformers (GPTs). Modern LLMs can be fine-tuned for specific tasks or guided using prompt engineering to achieve desired outcomes.\n",
            "\n",
            "These models gain predictive power in understanding syntax, semantics, and ontologies inherent in human language corpora. However, they also inherit inaccuracies and biases present in the data they are trained on. This means that while they can perform complex language tasks, they may also reflect and perpetuate existing biases and errors found in the training data.\n",
            "\n",
            "Please evaluate the response based on the following criteria:\n",
            "1 - Poor, 2 - Fair, 3 - Good, 4 - Very Good, 5 - Excellent\n",
            "Enter the relevance and coherence score (1-5): 1\n",
            "Evaluator Score: 1\n",
            "Rankings      : 21\n",
            "Score history :  1.48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.4. Human-expert evaluation\n",
        "\n",
        "Metrics such as cosine similarity indeed measure similarity but not provide in-depth accuracy. Time performance will not determine the accuracy of a response either. But if the rating is too low, why is that? Because the user is not satisfied with the response!"
      ],
      "metadata": {
        "id": "I_RPABQB_wIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters to trigger an expert’s feedback are counter_threshold and score_threshold\n",
        "# The number of user ratings must exceed the expert's threshold counter, which is counter_threshold=10\n",
        "# The threshold of the mean score of the ratings is 4 in this scenario: score_threshold=4\n",
        "counter_threshold=10\n",
        "score_threshold=4\n",
        "\n",
        "if counter > counter_threshold and score_history<=score_threshold:\n",
        "  print(\"Human expert evaluation is required for the feedback loop.\")\n",
        "\n",
        "# the output will confirm the expert feedback loop because of the poor mean ratings and the number of times the users rated the response"
      ],
      "metadata": {
        "id": "lz91VI2E94Fb"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now a standard HTML interface in a Python cell will display the thumbs-up and\n",
        "thumbs-down icons. If the expert presses on the thumbs-down icon, a feedback snippet can be entered and saved in a feedback file named expert_feedback.txt"
      ],
      "metadata": {
        "id": "m-4hxBmKtJ2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "from google.colab import output\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def image_to_data_uri(file_path):\n",
        "  # convert an image to data URI\n",
        "  with open(file_path, 'rb') as image_file:\n",
        "    encoded_string = base64.b64encode(image_file.read()).decode()\n",
        "  return f'data:image/png;base64,{encoded_string}'\n",
        "\n",
        "thumbs_up_data_uri = image_to_data_uri(\"drive/MyDrive/thumbs_up.png\")\n",
        "thumbs_down_data_uri = image_to_data_uri(\"drive/MyDrive/thumbs_down.png\")\n",
        "\n",
        "def display_icons():\n",
        "  # Define the HTML content with the two clickable images\n",
        "  html = f'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    '''\n",
        "  display(HTML(html))\n",
        "\n",
        "def save_feedback(feedback):\n",
        "  with open(\"drive/MyDrive/expert_feedback.txt\", \"w\") as file:\n",
        "    file.write(feedback)\n",
        "  print(\"Feedback saved successfully.\")\n",
        "\n",
        "# Register the callback\n",
        "output.register_callback('notebook.save_feedback', save_feedback)\n",
        "\n",
        "print(\"Human Expert Adaptive RAG activated\")\n",
        "\n",
        "# Display the icons with click handlers\n",
        "display_icons()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "furmx8JHLwWl",
        "outputId": "8a91a7dd-23b4-4bff-d2c3-1dcbb0d091de"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human Expert Adaptive RAG activated\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can add a function for thumbs-down meaning that the response was incorrect and that the management team has to communicate with the user panel or add a prompt to the user feedback interface. This is a management decision, of course. In our scenario, the human expert pressed the thumbs-down icon and was prompted to enter a response.\n",
        "\n",
        "The human expert provided the response, which was saved in 'drive/MyDrive/expert_feedback.txt'. The preceding expert's feedback can then be used to improve the RAG dataset"
      ],
      "metadata": {
        "id": "iUv3XMWwwtDU"
      }
    }
  ]
}